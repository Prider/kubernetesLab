Instruction for Workshop 2.5 Kubernetes in RealWorld:
Note: This instruction will start lab for kubernetes for real workshop:
====================================================
Lab Description:
VMName:							Machine name		Roles:						IP Address:
kubernetes-ms					kubernetes-ms		Master						192.168.99.200
kubernetes-1					kubernetes-1		NodePort					192.168.99.201
kubernetes-2					kubernetes-2		NodePort					192.168.99.202
===================================================
Username: kubeadmin
Password: kubeadmin
===================================================
0. Check region and zone current on GCloud (Note: your zone prefer, project code):
gcloud compute regions list
gcloud compute zones list
gcloud projects list

1. Setup project name "KubernetesProject" and set region/zone to gcloud client:
gcloud config set project kubernetesproject-170714
gcloud config set compute/region asia-east1
gcloud config set compute/zone asia-east1-a

2. Create Network/Firewall for KubernetesLab Group:
./initialnetwork.sh

3. Create Machine for KubernetesLab:
./initialcluster.sh

4. Check state of internet/external ip address (Note for all ip related):
gcloud compute instances list
------------------------------------
Sample Output
------------------------------------
NAME           ZONE          MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP     EXTERNAL_IP      STATUS
kubernetes-1   asia-east1-a  n1-standard-1               192.168.99.201  35.194.212.50    RUNNING
kubernetes-2   asia-east1-a  n1-standard-1               192.168.99.202  104.155.198.38   RUNNING
kubernetes-ms  asia-east1-a  n1-standard-1               192.168.99.200  104.199.190.131  RUNNING
------------------------------------

5. Copy script from local to googlecloud:
gcloud compute scp ./initialsetup1.8.sh kubernetes-ms:/home/praparn/initialsetup1.8.sh
gcloud compute scp ./dashboard-admin.yaml kubernetes-ms:/home/praparn/dashboard-admin.yaml
gcloud compute scp ./initialsetup1.8.sh kubernetes-1:/home/praparn/initialsetup1.8.sh
gcloud compute scp ./initialsetup1.8.sh kubernetes-2:/home/praparn/initialsetup1.8.sh

6. Open ssh (Windows by puttty) with 3 session:
gcloud compute ssh "kubernetes-ms"
gcloud compute ssh "kubernetes-1"
gcloud compute ssh "kubernetes-2"

7. For each node initial installation by command:
sudo apt-get update
chmod +x ./initialsetup1.8.sh
./initialsetup1.8.sh

******Logoff/Logon all node again*****
gcloud compute ssh "kubernetes-ms"
gcloud compute ssh "kubernetes-1"
gcloud compute ssh "kubernetes-2"
**************************************


8. For each node modify iptables by command:
	8.0: sudo su -
	8.1: iptables-save > /etc/iptables.conf
	8.2: vi /etc/iptables.conf ==> change ":FORWARD DROP [0:0]" to ":FORWARD ACCEPT [0:0]"
	8.3: iptables-restore < /etc/iptables.conf
	
9. (kubernetes-ms) initial cluster by command:
	sudo su -
	kubeadm init --kubernetes-version=v1.8.0 --pod-network-cidr=10.244.0.0/16 --token 8c2350.f55343444a6ffc46 --apiserver-cert-extra-sans <public ip>
	exit

	*Remark: output of this command will generate token that need to keep:
	-------------------------------------------------
	Sample Output:
	-------------------------------------------------
	[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.8.0
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.09.0-ce. Max validated version: 17.03
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [kubernetes-ms kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.99.200 104.199.190.131]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "scheduler.conf"
[controlplane] Wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[controlplane] Wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests"
[init] This often takes around a minute; or longer if the control plane images have to be pulled.
[apiclient] All control plane components are healthy after 38.501613 seconds
[uploadconfig]Â Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[markmaster] Will mark node kubernetes-ms as master by adding a label and a taint
[markmaster] Master kubernetes-ms tainted and labelled with key/value: node-role.kubernetes.io/master=""
[bootstraptoken] Using token: 8c2350.f55343444a6ffc46
[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: kube-dns
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run (as a regular user):

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node
as root:
  kubeadm join --token 8c2350.f55343444a6ffc46 192.168.99.200:6443 --discovery-token-ca-cert-hash sha256:3ad6af0ae754b8ec5bb6d02d11766b3aa12fbbf2e77a480befe02419ed45542b
  
	-------------------------------------------------
10. (kubernetes-ms) Setup run cluster system by command (Regular User):
		  mkdir -p $HOME/.kube
		  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
		  sudo chown $(id -u):$(id -g) $HOME/.kube/config
		  kubectl taint nodes --all node-role.kubernetes.io/master-

11. (local) SCP Certificate from Google Cloud to Local:
	gcloud compute scp kubernetes-ms:/home/<home directory>/.kube/config adminconfig.conf

12. (kubernetes-ms) Create Flennel net plugin for network for cluster by command:
	kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
    kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.8.0/Documentation/kube-flannel-rbac.yml

13. (kubernetes-ms) Check master readyness and dns by command (Take 5 - 10 min):
	kubectl get pods --all-namespaces
	kubectl describe pods <kube-dns name> --namespace kube-system

	-------------------------------------------------
	Sample Output
	-------------------------------------------------
	NAMESPACE     NAME                                   READY     STATUS    RESTARTS   AGE
kube-system   etcd-kubeserve-ms                      1/1       Running   0          1m
kube-system   kube-apiserver-kubeserve-ms            1/1       Running   0          2m
kube-system   kube-controller-manager-kubeserve-ms   1/1       Running   0          3m
kube-system   kube-dns-2425271678-xhhk6              3/3       Running   0          2m
kube-system   kube-flannel-ds-n9ws7                  2/2       Running   0          2m
kube-system   kube-proxy-vzswv                       1/1       Running   0          2m
kube-system   kube-scheduler-kubeserve-ms            1/1       Running   0          2m
	-------------------------------------------------

14. (local) Edit file adminconfig.conf for change ip address from private to public ip:
vi adminconfig.conf

15. (local) Configure local kubectl for access and command cluster:
kubectl --kubeconfig ./adminconfig.conf get nodes
kubectl --kubeconfig ./adminconfig.conf get svc

16. (kubenetes-ms) Install dashboard by command:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
kubectl create -f dashboard-admin.yaml
kubectl get pods --all-namespaces

17. (local) Open dashboard by command:
kubectl --kubeconfig ./adminconfig.conf proxy --accept-hosts '.*'

18. (local) Open browser by command
http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/

19. (kubernetes-1),(kubernetes-2) ssh and join to cluster by command:
	sudo su -
	kubeadm join --token 8c2350.f55343444a6ffc46 192.168.99.200:6443 --discovery-token-ca-cert-hash sha256:3ad6af0ae754b8ec5bb6d02d11766b3aa12fbbf2e77a480befe02419ed45542b
	exit
	-------------------------------------------------
	Sample Output
	-------------------------------------------------
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[preflight] Running pre-flight checks
[preflight] Some fatal errors occurred:
	user is not running as root
[preflight] If you know what you are doing, you can skip pre-flight checks with `--skip-preflight-checks`
kubeadmin@kubeserve-1:~$ sudo kubeadm join --token 78435a.f1eb236036e232ff 192.168.99.200:6443
[sudo] password for kubeadmin:
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[preflight] Running pre-flight checks
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.06.0-ce. Max validated version: 1.12
[discovery] Trying to connect to API Server "192.168.99.200:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://192.168.99.200:6443"
[discovery] Cluster info signature and contents are valid, will use API Server "https://192.168.99.200:6443"
[discovery] Successfully established connection with API Server "192.168.99.200:6443"
[bootstrap] Detected server version: v1.7.0
[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)
[csr] Created API client to obtain unique certificate for this node, generating keys and certificate signing request
[csr] Received signed certificate from the API server, generating KubeConfig...
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"

Node join complete:
* Certificate signing request sent to master and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on the master to see this machine join.
	-------------------------------------------------

20. (kubernetes-ms) Check Node in Cluster by command (This take 5 - 10 min):
kubectl get node

-------------------------------------------------
Sample Output
-------------------------------------------------
NAME           STATUS    AGE       VERSION
kubeserve-1    Ready     5m        v1.7.0
kubeserve-2    Ready     3m        v1.7.0
kubeserve-ms   Ready     19m       v1.7.0
-------------------------------------------------

21. (kubernetes-ms)Check Pods from all cluster system running by command:
kubectl get pods --all-namespaces

-------------------------------------------------
Sample Output
-------------------------------------------------
NAMESPACE     NAME                                   READY     STATUS    RESTARTS   AGE
kube-system   etcd-kubeserve-ms                      1/1       Running   0          10m
kube-system   kube-apiserver-kubeserve-ms            1/1       Running   0          10m
kube-system   kube-controller-manager-kubeserve-ms   1/1       Running   0          11m
kube-system   kube-dns-2425271678-xhhk6              3/3       Running   0          11m
kube-system   kube-flannel-ds-1h8cv                  2/2       Running   2          7m
kube-system   kube-flannel-ds-83xdr                  2/2       Running   0          6m
kube-system   kube-flannel-ds-n9ws7                  2/2       Running   0          11m
kube-system   kube-proxy-6d7g1                       1/1       Running   0          7m
kube-system   kube-proxy-qzfdr                       1/1       Running   0          6m
kube-system   kube-proxy-vzswv                       1/1       Running   0          11m
kube-system   kube-scheduler-kubeserve-ms            1/1       Running   0          11m
-------------------------------------------------

22. (all node) configure iptable for forward packet from host to container by command:
sudo iptables -P FORWARD ACCEPT

23. (kubernetes-ms) Test deployment basic nginx pods by command
kubectl run webtest --image=labdocker/nginx:latest --port=80
kubectl get pods -o wide
kubectl expose deployment webtest --target-port=80 --type=NodePort
kubectl get svc -o wide

24. (kubernetes-ms) Test get web inside farm by curl:
curl http://192.168.99.200:<port>
curl http://192.168.99.201:<port>
curl http://192.168.99.202:<port>

25. (Client Machine) Test get web outside farm by curl or browser:
gcloud compute instances list
------------------------------------
Sample Output
------------------------------------
NAME           ZONE          MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP     EXTERNAL_IP      STATUS
kubernetes-1   asia-east1-a  n1-standard-1               192.168.99.201  104.199.230.125  RUNNING
kubernetes-2   asia-east1-a  n1-standard-1               192.168.99.202  35.185.141.171   RUNNING
kubernetes-ms  asia-east1-a  n1-standard-1               192.168.99.200  35.185.168.113   RUNNING
------------------------------------

curl http://<External IP of MS>:<port>
curl http://<External IP of MS>:<port>
curl http://<External IP of MS>:<port>

26. (kubeserve-ms) Cleanup Lab by command:
kubectl delete deployment/webtest
kubectl delete svc/webtest